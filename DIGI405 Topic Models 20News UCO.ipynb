{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c2ff85-b251-4dd1-9c69-40af07976cc0",
   "metadata": {},
   "source": [
    "# DIGI405 - UC Online - Topic Models with tomotopy \n",
    "This notebook explores LDA topic modeling with the tomotopy library. Tomotopy is a Python extension of *tomoto* (*to*pic *mo*deling *to*ol), which is a [Gibbs-sampling](https://www.youtube.com/watch?v=BaM1uiCpj_E) based topic model library written in C++. \n",
    "\n",
    "Read more in the tomotopy documentation... \n",
    "## <img src=https://bab2min.github.io/tomotopy/tomoto.png align=\"left\" width=\"20\">[**tomotopy**](https://bab2min.github.io/tomotopy/v0.12.2/en/)\n",
    "\n",
    "You might think about how the output of this topic model might be used. You will see that some topics cover documents from multiple newsgroups - how could this be useful? How does a distribution of topics differ from having a single label?\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Note:</strong> This is optional / bonus material, but there are some tasks and questions for you to reflect on as you go. Complete these and make some notes to get the most out of the notebook. Tasks or questions will have a box around them like this!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-machinery",
   "metadata": {},
   "source": [
    "## Import the necessary Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860bc2b-9ac7-495a-b9a4-e9ede8de88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# various packages\n",
    "from zipfile import ZipFile\n",
    "import os.path\n",
    "from os import path\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "# from collections import Counter, Iterable\n",
    "import datetime\n",
    "\n",
    "# topic modeling / nlp packages\n",
    "import tomotopy as tp\n",
    "from tomotopy.utils import Corpus\n",
    "\n",
    "# visualisation / exploration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tmplot\n",
    "from IPython.display import IFrame, Markdown, display\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# These last lines of code suppress deprecation warnings displaying in the notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-madrid",
   "metadata": {},
   "source": [
    "### Install Python packages if needed\n",
    "\n",
    "If you get an import error, it is likely because you need to install tomotopy or tmplot. Uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac217a1-8cc8-424b-8434-41829f859ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tomotopy tmplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293d2e8-4f3f-4b58-a329-96a861fd5b0e",
   "metadata": {},
   "source": [
    "Run to download NLTK stopwords and functions for pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e32bb18-a0a3-4098-898b-dcab478603f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-deficit",
   "metadata": {},
   "source": [
    "## Define functions\n",
    "\n",
    "The following cell contains a function to preprocess the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c66c2-df0b-43bc-9f61-129eb0ec31f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set, extra_stopwords={}):\n",
    "    # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
    "    \n",
    "    # replace all newlines or multiple sequences of spaces with a standard space\n",
    "    doc_set = [re.sub(r'\\s+', ' ', doc) for doc in doc_set]\n",
    "    \n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    \n",
    "    # add any extra stopwords\n",
    "    if (len(extra_stopwords) > 0):\n",
    "        en_stop = en_stop.union(extra_stopwords)\n",
    "    \n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        filtered_tokens = [token for token in tokens if token.isalpha() and len(token) > 2]\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [token for token in filtered_tokens if not token in en_stop]\n",
    "        # add tokens to list\n",
    "        texts.append(stopped_tokens)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660cc95a-b5a4-4aa9-8df4-16fbe91317af",
   "metadata": {},
   "source": [
    "## Load and pre-process the corpus\n",
    "\n",
    "Load the 20 Newsgroups corpus and preprocess it. After tokenising, removing non-alphanumeric tokens, 2-letter words and NLTK stopwords, we also remove short documents that contribute fewer than 5 words to our 'bag of words'. We also (hopefully) remove duplicate posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a704f8e-82b2-415e-bb97-80166574fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T\n",
    "df.columns = ['text', 'target']\n",
    "df['bow'] = doc_clean = preprocess_data(pd.Series(df['text']))\n",
    "targets = pd.DataFrame(newsgroups_train.target_names)\n",
    "targets.columns=['newsgroup_title']\n",
    "df = pd.merge(df, targets, left_on='target', right_index=True)\n",
    "df = df.drop_duplicates(subset='text')\n",
    "df = df[df['bow'].map(len) > 5] # remove empty or short documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20de8e4-288f-4c35-9de7-98cd292e23d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the processed data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a2a1fe-4e05-491f-8538-98753931c668",
   "metadata": {},
   "source": [
    "## Set parameters\n",
    "In the cell below you can set the parameters of the LDA topic model. Leave them as default the first time you train the model. More information about the tomotopy LDA model parameters can be found [here](https://bab2min.github.io/tomotopy/v0.4.1/en/#tomotopy.LDAModel).\n",
    "\n",
    "* α – alpha, a Dirichlet prior on the per-document topic distribution\n",
    "* β – beta / eta, a Dirichlet prior on the per-topic word distribution\n",
    "* optim_interval - how often to optimise the beta hyperparameter\n",
    "* k – the number of topics in the model\n",
    "* burn-in – the number of burn-in iterations\n",
    "* iter – the number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd953a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum frequency of words (integer)\n",
    "# Words with a smaller document frequency than min_df are excluded from the model\n",
    "# Default is 0 - i.e. no words are excluded\n",
    "# For more info see https://bab2min.github.io/tomotopy/v0.12.3/en/#vocabulary-controlling-using-cf-and-df\n",
    "min_doc_freq = 5\n",
    "\n",
    "# Number of top words to be removed (integer)\n",
    "# Setting this to 1 or more removes common words from the model\n",
    "# Default is 0 - i.e. no words are excluded\n",
    "remove_top = 0\n",
    "\n",
    "# Number of topics to return, between 1 and 32767\n",
    "num_topics = 20\n",
    "\n",
    "# You can read more about the following alpha and beta hyperparameters here:\n",
    "# https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "\n",
    "# Alpha\n",
    "# Hyperparameter of Dirichlet distribution for document-topic\n",
    "# Controls the density of topics per document\n",
    "# a float\n",
    "doc_topic = 0.1\n",
    "\n",
    "# Beta\n",
    "# Hyperparameter of Dirichlet distribution for topic-word\n",
    "# Note this is 'eta' in tomotopy - it's not a typo!\n",
    "# Controls the density of words per topic\n",
    "# a float\n",
    "topic_word = 0.01\n",
    "\n",
    "# Set the burn-in\n",
    "# Number of initial iterations that are discarded before optimising hyperparameters\n",
    "# This speeds up the convergence of the model on an optimal set of topics\n",
    "brn_in = 10\n",
    "\n",
    "# Number of iterations of the Gibbs sampler\n",
    "# If we specify 30 here, we will run 300 (10*30) iterations of Gibbs sampling total in the training loop\n",
    "num_iterations = 30\n",
    "\n",
    "# Set the top n words from the topic to display in the output of results\n",
    "num_topic_words = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-forwarding",
   "metadata": {},
   "source": [
    "## Train the model and display the results\n",
    "Run the cell below to train the model and display the results (you shouldn't need to change anything here). **Important:** Each time you change settings of the LDA model, you will need to re-run the cell below to re-train the model. Because LDA is unsupervised and probabalistic, it will produce different results each time. We can control this using a random seed (the `seed` parameter below), but it is worth remembering that these models are very sensitive to changes in their input and we should think of them as approximations of some latent topics, and there is really no single 'correct' model.\n",
    "\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 1:</strong> Look through the output. Consider the impact of the different parameter settings on the results. There are several ways to optimise and tweak the results. Try to:\n",
    "    \n",
    "- test more or fewer topics.\n",
    "- test more or fewer number of iterations\n",
    "- test the `min_doc_freq` and `remove_top` variables\n",
    "    \n",
    "Which parameter settings produce the clearest topic groupings?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda3d45c",
   "metadata": {},
   "source": [
    "_Your answers here..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289d97b-63e2-4011-a51f-80e086333740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://bab2min.github.io/tomotopy/v0.12.2/en/\n",
    "\n",
    "# Intialize the model\n",
    "\n",
    "# The default term weighting (\"ONE\") is used below - all terms are weighted equally\n",
    "# \"PMI\" - Pointwise mutual information - or \"IDF\" - inverse document frequency - can also be used for term weighting\n",
    "model = tp.LDAModel(tw=tp.TermWeight.ONE,\n",
    "                    min_df=min_doc_freq, \n",
    "                    rm_top=remove_top, \n",
    "                    k=num_topics, \n",
    "                    alpha=doc_topic, \n",
    "                    eta=topic_word,\n",
    "                    seed=77,\n",
    "                   )\n",
    "\n",
    "model.burn_in = brn_in\n",
    "\n",
    "# Add each document to the model\n",
    "for text in df['bow']:\n",
    "    model.add_doc(text)\n",
    "\n",
    "print(\"Topic Model Training...\\n\")\n",
    "\n",
    "# train the model\n",
    "# the loop reports LL/word every 10 iterations\n",
    "# this is a measure of model fit to the data (higher is better)\n",
    "for i in range(0, 10):\n",
    "    model.train(iter=num_iterations)\n",
    "\n",
    "topics = []\n",
    "topic_individual_words = []\n",
    "for topic_number in range(0, num_topics):\n",
    "    topic_words = ' '.join(word for word, prob in model.get_topic_words(topic_id=topic_number, top_n=num_topic_words))\n",
    "    print(f'\\nTop 10 words of topic #{topic_number}\\n')\n",
    "    print(model.get_topic_words(topic_id=topic_number, top_n=num_topic_words))\n",
    "    print()\n",
    "    topics.append(topic_words)\n",
    "    topic_individual_words.append(topic_words.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7beb7b-202b-41d7-8194-14008c51d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel Summary\\n\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the frequent words removed by rm_top. \n",
    "# A useful filter, esp with \"ONE\" term weighting\n",
    "model.removed_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b1684-fa7e-4765-8287-df2f616e4946",
   "metadata": {},
   "source": [
    "## Visualise the topic model\n",
    "\n",
    "We are using the tmplot visualisation library, which is strongly influenced by pyLDAvis and the R library LDAvis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a6e963-b0cd-4c20-b944-c9b3cd991828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a truncated list of docs for display in tmplot\n",
    "trunc_docs = [doc[:400] for doc in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4aa5e-2e73-4b87-b4a9-db75aebd5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we supply tmplot's list of docs\n",
    "tmplot.report(model, docs=trunc_docs, width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-incentive",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 2:</strong> Take a few minutes to explore the interactive visualisation to explore the topics and see whether the top documents for each topic make sense. Are there good / bad topics here? Make some notes. You can also adjust the &lambda;(Lambda) value if you wish. This is another metric that allows you explore words in topics along a scale between being weighted entirely by the probability of the word given the topic (if &lambda; = 1), to weighted entirely by the marginal term probability (ie relative frequency) of the word in the corpus (if &lambda; = 0). What value of &lambda; filters the topics best?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2afbf4",
   "metadata": {},
   "source": [
    "_Your answer here..._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bf53d-176a-462b-a756-b1db0217b782",
   "metadata": {},
   "source": [
    "## Examine top documents for a given topic\n",
    "The following code to display the top documents is adapated from [***Topic Modeling - With Tomotopy***](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/09-Topic-Modeling-Without-Mallet.html) from the book *Introduction to Cultural Analytics & Python* by Melanie Walsh (2021). Because of the messiness of the 20 Newsgroups dataset, some documents may not display correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca5faf-d7bd-487e-a716-fe76434bfeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions = [list(doc.get_topic_dist()) for doc in model.docs]\n",
    "\n",
    "topic_indiv_150_words = []\n",
    "for topic_number in range(0, num_topics):\n",
    "    topic_words_150 = ' '.join(word for word, prob in model.get_topic_words(topic_id=topic_number, top_n=150))\n",
    "    topic_indiv_150_words.append(topic_words_150.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974316dc-6ffb-401c-be8c-3a3cf0e94302",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "def make_md(string):\n",
    "    display(Markdown(str(string)))\n",
    "\n",
    "def get_top_docs(docs, sources, topic_indiv_150_words, topic_distributions, topic_index, n):\n",
    "\n",
    "    sorted_data = sorted([(_distribution[topic_index], _document, _sources) \n",
    "                          for _distribution, _document, _sources\n",
    "                          in zip(topic_distributions, docs, sources)], reverse=True)\n",
    "\n",
    "    \n",
    "    top_25 = \", \".join(topic_indiv_150_words[topic_index][:25])\n",
    "    make_md\\\n",
    "    (f\"### Topic {topic_index}\\n\\\n",
    "    \\n{top_25} ...\\\n",
    "    \\n\\n**Note**: the highest ranking 150 words in the topic are shown in bold in each text\\n\\n---\")\n",
    "    \n",
    "    for proportion, doc, source in sorted_data[:n]:\n",
    "        # Make topic words bolded\n",
    "        for word in topic_indiv_150_words[topic_index]:\n",
    "            #doc = doc.lower()\n",
    "            if word in doc:\n",
    "                doc = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", doc)\n",
    "                #doc = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", doc, re.IGNORECASE)\n",
    "        \n",
    "        make_md(f'  \\n**Topic Proportion**: {proportion}  \\n**Source**: {source}  \\n**Document**: {doc}  \\n\\n---')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd04baae-6fc2-4f76-8e29-dbab4ac786c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the topic number to explore\n",
    "topic_no = 9\n",
    "\n",
    "# Set the number of 'top docs' to display\n",
    "# Some newsgroups posts are long!\n",
    "num_top_docs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7273b-32ac-4de4-b886-4ff4005ff29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top documents for the selected topic, with topic words highlighted\n",
    "\n",
    "get_top_docs(df['text'], \n",
    "             df['newsgroup_title'],\n",
    "             topic_indiv_150_words, \n",
    "             topic_distributions, \n",
    "             topic_index = topic_no, \n",
    "             n = num_top_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0aa79a",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 3:</strong> Choose a topic and note some of its characteristics below: what is the topic about? Do documents from multiple newsgroups appear in it? Are there words in the topic that clearly fit or do not fit together?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212a16a",
   "metadata": {},
   "source": [
    "_Your description of a topic here..._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (ipykernel)",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
